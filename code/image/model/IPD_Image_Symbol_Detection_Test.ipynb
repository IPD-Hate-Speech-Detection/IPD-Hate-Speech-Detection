{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T855IGfxzOEv",
        "outputId": "bcc25179-00b8-4a30-e99f-7c963452a99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_path = '/content/drive/MyDrive/IPD/code/image/hate_symbols/data'\n",
        "train_path = '/content/drive/MyDrive/IPD/code/image/hate_symbols/train'\n",
        "valid_path = '/content/drive/MyDrive/IPD/code/image/hate_symbols/valid'"
      ],
      "metadata": {
        "id": "GHM_RhiZzRn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for dir in os.listdir(data_path):\n",
        "#   new_folder_path = os.path.join(train_path, dir)\n",
        "#   new_folder_path = os.path.join(valid_path, dir)\n",
        "#   os.makedirs(new_folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "6G2-zkvf0fgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import os\n",
        "\n",
        "# for dir in os.listdir(data_path):\n",
        "#   new_folder_path = os.path.join(train_path, dir)\n",
        "#   for file in os.listdir(os.path.join(data_path,dir)):\n",
        "#     try:\n",
        "#       img = Image.open(os.path.join(data_path,dir,file))\n",
        "#       img = img.resize((224,224))\n",
        "#       rgb_img = img.convert('RGB')\n",
        "#       rgb_img.save(os.path.join(new_folder_path,file.split('.')[0]+'.jpg'))\n",
        "#     except:\n",
        "#       pass"
      ],
      "metadata": {
        "id": "gmJzDLMP1i3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "num_classes = 5\n",
        "image_size = (224, 224)\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=image_size + (3,))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "steps_per_epoch = len(train_generator)//batch_size\n",
        "validation_steps = len(val_generator)//batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=validation_steps\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtUG2vH_I1GT",
        "outputId": "98174b80-0323-4ccb-9833-be9384c540fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1043 images belonging to 5 classes.\n",
            "Found 258 images belonging to 5 classes.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 7s/step - accuracy: 0.4180 - loss: 3.5732 - val_accuracy: 0.5938 - val_loss: 3.7232\n",
            "Epoch 2/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 6s/step - accuracy: 0.5497 - loss: 2.1819 - val_accuracy: 0.0625 - val_loss: 828.8075\n",
            "Epoch 3/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 6s/step - accuracy: 0.5457 - loss: 1.2973 - val_accuracy: 0.4062 - val_loss: 1707.5105\n",
            "Epoch 4/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 6s/step - accuracy: 0.6573 - loss: 1.2152 - val_accuracy: 0.6562 - val_loss: 2.1774\n",
            "Epoch 5/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 6s/step - accuracy: 0.6805 - loss: 1.1069 - val_accuracy: 0.6250 - val_loss: 1.7910\n",
            "Epoch 6/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 6s/step - accuracy: 0.7083 - loss: 1.0729 - val_accuracy: 0.6250 - val_loss: 1.5045\n",
            "Epoch 7/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 6s/step - accuracy: 0.7586 - loss: 0.9042 - val_accuracy: 0.5312 - val_loss: 1.7347\n",
            "Epoch 8/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 6s/step - accuracy: 0.6903 - loss: 0.9976 - val_accuracy: 0.6562 - val_loss: 2.9561\n",
            "Epoch 9/10\n",
            "\u001b[1m 3/16\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 6s/step - accuracy: 0.5486 - loss: 1.2499"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.5768 - loss: 1.1729 - val_accuracy: 0.5000 - val_loss: 1.6920\n",
            "Epoch 10/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 6s/step - accuracy: 0.5390 - loss: 1.1935 - val_accuracy: 0.6250 - val_loss: 3.7649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('symbol_detector_resnet.keras')"
      ],
      "metadata": {
        "id": "ncnmS1tf4FNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_indices = train_generator.class_indices\n",
        "index_to_class = {v: k for k, v in class_indices.items()}\n",
        "print(index_to_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B-wnnUhhxqT",
        "outputId": "dca1192d-e61f-46c6-c2c5-b1714eff1591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'cut_throat_gesture', 1: 'finger_gun_to_the_head', 2: 'middle_finger', 3: 'slanted_eyes_gesture', 4: 'swastika'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_image(image_path, model, image_size):\n",
        "    img = load_img(image_path, target_size=image_size)\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    print(predictions)\n",
        "    predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return predicted_class, predictions\n",
        "\n",
        "image_path = 'hate_symbol.jpeg'\n",
        "predicted_class, predictions = predict_single_image(image_path, model, image_size)\n",
        "print(f\"Predicted Class: {predicted_class}\")\n",
        "print(f\"Predictions: {predictions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT5sPL73f-JT",
        "outputId": "9ec39d2d-21c1-4606-fc0a-4825cf97de05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "[[4.4611324e-02 9.5517528e-01 2.0699757e-04 5.6824829e-06 7.3142149e-07]]\n",
            "Predicted Class: [1]\n",
            "Predictions: [[4.4611324e-02 9.5517528e-01 2.0699757e-04 5.6824829e-06 7.3142149e-07]]\n"
          ]
        }
      ]
    }
  ]
}